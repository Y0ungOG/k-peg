{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4uPki6O9oTq"
      },
      "source": [
        "Deadline: Dec 10 2023\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Under mild theoretical assumptions (non existence of arbitrage opportunities), it can be shown that the time $t$ price $P_t$ of any asset that pays $X_{t+1}$ at time $t+1$ has to satisfy the equation:\n",
        "$$\n",
        "    P_t = \\mathbb{E}_t\\left[M_{t + 1} X_{t + 1} \\right]\n",
        "$$\n",
        "where $M_{t + 1}$ is a stochastic variable called Stochastic Discount Factor.\n",
        "\n",
        "For the particular case of a stock, we can write it's future payoff $X_{t + 1}$as:\n",
        "\n",
        "$$\n",
        "    X_{t + 1} = D_{t + 1} + P_{t + 1}\n",
        "$$,\n",
        "\n",
        "where $D_{t + 1}$ is the dividend payed next year and $P_{t + 1}$ is the stock price next year. That is, if you buy 1 stock now and sell it exactly one year from now your cash flows are $-P_t$ today and $D_{t + 1} + P_{t + 1}$ in one year. These cashflows have to satisfy:\n",
        "\n",
        "$$\n",
        "    P_t = \\mathbb{E}_t\\left[M_{t + 1} (D_{t + 1} + P_{t + 1}) \\right]\n",
        "$$\n",
        "\n",
        "A common assumption to study asset prices is that the Stochastic Discount Factor can be decomposed as:\n",
        "\n",
        "$$\n",
        "    M_{t + 1} = \\gamma \\frac{\\pi_{t+1}}{\\pi_t}\n",
        "$$\n",
        "\n",
        "where $\\pi$ is a random variable that captures the state of the economy and $\\gamma$ is a constant $\\in [0, 1)$. Under this assumption, stock prices have to satisfy:\n",
        "\n",
        "$$\n",
        "    \\pi_t P_t = \\mathbb{E}_t\\left[\\gamma   \\pi_{t + 1} (D_{t + 1} + P_{t + 1}) \\right]\n",
        "$$\n",
        "\n",
        "Defining\n",
        "$$V\\equiv \\pi P$$\n",
        "\n",
        "and\n",
        "\n",
        "$$R\\equiv \\gamma   \\pi_{t + 1} D_{t + 1}$$\n",
        "we finally get:\n",
        "\n",
        "$$\n",
        "V_t = \\mathbb{E}_t\\left[R + \\gamma V_{t+1} \\right]\n",
        "$$\n",
        "\n",
        "that is, stock prices satisfy a Bellman equation!\n",
        "\n",
        "Use a neural network to represent the value function $V$ and solve the corresponding Bellman equation using temporal differences learning (TD).\n",
        "Once you have solved for $V$, you can back out the stock price as:\n",
        "\n",
        "$$\n",
        "    P_t = \\frac{V_t}{\\pi_t}\n",
        "$$\n",
        "\n",
        "Plot the stock price as a function of the dividends.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQxRbKVqDtHE"
      },
      "source": [
        "# Data\n",
        "\n",
        "You have a time series of dividends payed by a given *firm*. Suppose you also have a time series for $\\pi$. The data is stored in the file replay_buffer.npy, which is a $10000 \\times 4$ matrix where each row consists of an observation of:\n",
        "\n",
        "$$\n",
        "    D_t, D_{t + 1}, \\pi_t, \\pi_{t+1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTJDA7v1FiYA"
      },
      "source": [
        "%%capture\n",
        "# This snipped of code will download the data file\n",
        "!wget https://www.dropbox.com/s/ms5p2hqqhlaago7/replay_buffer.npy?dl=1\n",
        "!mv replay_buffer.npy?dl=1 replay_buffer.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download npy\n",
        "import numpy as np\n",
        "data = np.load('/replay_buffer.npy')\n",
        "data_matrix = np.asarray(data)\n",
        "print(data_matrix)"
      ],
      "metadata": {
        "id": "aZe6hhvFKeox",
        "outputId": "9c100e4d-7b00-40bf-9950-6be9282ecd32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.7544187  1.6899673  0.32488784 0.35014135]\n",
            " [1.5905526  1.617272   0.39527917 0.382326  ]\n",
            " [1.6724763  1.6786338  0.3575033  0.35488534]\n",
            " ...\n",
            " [1.4697709  1.3415053  0.4629144  0.55566776]\n",
            " [2.2108552  2.2101867  0.20458762 0.20471142]\n",
            " [1.322186   1.2165726  0.5720248  0.67565334]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Bellman Loss\n",
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss, self).__init__()\n",
        "\n",
        "    def forward(self, input, target,R):\n",
        "        #Bellman loss\n",
        "        gamma=0.85\n",
        "        loss = torch.mean(torch.abs(gamma*target+gamma*R - input))\n",
        "        return loss\n",
        "\n",
        "# Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc = nn.Linear(5, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pP9TAnMuKkVv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "net = Net()\n",
        "\n",
        "loss_func = CustomLoss()\n",
        "# optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
        "\n",
        "# random input\n",
        "inputs = torch.randn(2000, 5)\n",
        "targets = torch.randn(2000, 5)\n",
        "result_vector = data_matrix[:, 1] * data_matrix[:, 3]\n",
        "R = torch.tensor(result_vector).view(2000, 5)\n",
        "\n",
        "# training\n",
        "epochs = 20\n",
        "batch_size = 100\n",
        "num_batches = inputs.size(0) // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch in range(num_batches):\n",
        "        batch_inputs = inputs[batch * batch_size: (batch + 1) * batch_size]\n",
        "        batch_targets = targets[batch * batch_size: (batch + 1) * batch_size]\n",
        "        batch_R = R[batch * batch_size: (batch + 1) * batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(batch_inputs)\n",
        "        loss = loss_func(outputs, batch_targets,batch_R)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, running_loss/num_batches))"
      ],
      "metadata": {
        "id": "Kis5KNtJv-_Z",
        "outputId": "8357b7d5-8f03-4b09-e42f-7cc6e2ed3fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 1.0293\n",
            "Epoch [2/20], Loss: 0.9204\n",
            "Epoch [3/20], Loss: 0.8493\n",
            "Epoch [4/20], Loss: 0.8050\n",
            "Epoch [5/20], Loss: 0.7795\n",
            "Epoch [6/20], Loss: 0.7651\n",
            "Epoch [7/20], Loss: 0.7574\n",
            "Epoch [8/20], Loss: 0.7536\n",
            "Epoch [9/20], Loss: 0.7517\n",
            "Epoch [10/20], Loss: 0.7506\n",
            "Epoch [11/20], Loss: 0.7501\n",
            "Epoch [12/20], Loss: 0.7498\n",
            "Epoch [13/20], Loss: 0.7496\n",
            "Epoch [14/20], Loss: 0.7496\n",
            "Epoch [15/20], Loss: 0.7495\n",
            "Epoch [16/20], Loss: 0.7495\n",
            "Epoch [17/20], Loss: 0.7494\n",
            "Epoch [18/20], Loss: 0.7494\n",
            "Epoch [19/20], Loss: 0.7494\n",
            "Epoch [20/20], Loss: 0.7494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V=net(inputs)\n",
        "pai = torch.tensor(data_matrix[:, 2]).view(2000, 5)\n",
        "P=torch.div(V,pai)\n",
        "P=P.view(10000,1)"
      ],
      "metadata": {
        "id": "DqdAJvpJ1Fds"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(P)"
      ],
      "metadata": {
        "id": "lC3w6u054Vxu",
        "outputId": "303672a4-8cfb-4488-d149-87f60ae62593",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.4498],\n",
            "        [1.7286],\n",
            "        [2.0523],\n",
            "        ...,\n",
            "        [1.5176],\n",
            "        [3.4367],\n",
            "        [1.1196]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}